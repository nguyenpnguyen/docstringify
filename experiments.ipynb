{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa8c3faf-d752-4c30-a83a-37ea37dc6eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ingest import load_and_split_repository, get_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb63428a-cc16-42ec-8f62-cb61716fa22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning repository: tests/sample...\n",
      "processed 32 code chunks.\n",
      "Chunk no: 0\n",
      "Content: \n",
      "class ModelConfig:\n",
      "    num_hidden_layers: int = 36\n",
      "    num_experts: int = 128\n",
      "    experts_per_token: int = 4\n",
      "    vocab_size: int = 201088\n",
      "    hidden_size: int = 2880\n",
      "    intermediate_size: int = 2880\n",
      "    swiglu_limit: float = 7.0\n",
      "    head_dim: int = 64\n",
      "    num_attention_heads: int = 64\n",
      "    num_key_value_heads: int = 8\n",
      "    sliding_window: int = 128\n",
      "    initial_context_length: int = 4096\n",
      "    rope_theta: float = 150000.0\n",
      "    rope_scaling_factor: float = 32.0\n",
      "    rope_ntk_alpha: float = 1.0\n",
      "    rope_ntk_beta: float = 32.0\n",
      "---\n",
      "Metadata: \n",
      "{'source': 'tests/sample/model.py', 'name': 'ModelConfig', 'type': 'class_definition', 'start_line': 13, 'docstring': None}\n",
      "\n",
      "****************************************\n",
      "\n",
      "Chunk no: 1\n",
      "Content: \n",
      "class RMSNorm(torch.nn.Module):\n",
      "    def __init__(\n",
      "        self, num_features: int, eps: float = 1e-05, device: torch.device | None = None\n",
      "    ):\n",
      "        super().__init__()\n",
      "        self.num_features = num_features\n",
      "        self.eps = eps\n",
      "        self.scale = torch.nn.Parameter(\n",
      "            torch.ones(num_features, device=device, dtype=torch.float32)\n",
      "        )\n",
      "---\n",
      "Metadata: \n",
      "{'source': 'tests/sample/model.py', 'name': 'RMSNorm', 'type': 'class_definition', 'start_line': 32, 'docstring': None}\n",
      "\n",
      "****************************************\n",
      "\n",
      "Chunk no: 2\n",
      "Content: \n",
      "def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
      "        assert x.shape[-1] == self.num_features\n",
      "        t, dtype = x.float(), x.dtype\n",
      "        t = t * torch.rsqrt(torch.mean(t**2, dim=-1, keepdim=True) + self.eps)\n",
      "        return (t * self.scale).to(dtype)\n",
      "---\n",
      "Metadata: \n",
      "{'source': 'tests/sample/model.py', 'name': 'forward', 'type': 'method', 'parent_class': 'RMSNorm', 'start_line': 43, 'docstring': None}\n",
      "\n",
      "****************************************\n",
      "\n",
      "Chunk no: 3\n",
      "Content: \n",
      "def _apply_rotary_emb(\n",
      "    x: torch.Tensor,\n",
      "    cos: torch.Tensor,\n",
      "    sin: torch.Tensor,\n",
      ") -> torch.Tensor:\n",
      "    cos = cos.unsqueeze(-2).to(x.dtype)\n",
      "    sin = sin.unsqueeze(-2).to(x.dtype)\n",
      "    x1, x2 = torch.chunk(x, 2, dim=-1)\n",
      "    o1 = x1 * cos - x2 * sin\n",
      "    o2 = x2 * cos + x1 * sin\n",
      "    return torch.cat((o1, o2), dim=-1)\n",
      "---\n",
      "Metadata: \n",
      "{'source': 'tests/sample/model.py', 'name': '_apply_rotary_emb', 'type': 'function', 'parent_class': None, 'start_line': 50, 'docstring': None}\n",
      "\n",
      "****************************************\n",
      "\n",
      "Chunk no: 4\n",
      "Content: \n",
      "class RotaryEmbedding(torch.nn.Module):\n",
      "    def __init__(\n",
      "        self,\n",
      "        head_dim: int,\n",
      "        base: int,\n",
      "        dtype: torch.dtype,\n",
      "        initial_context_length: int = 4096,\n",
      "        scaling_factor: float = 1.0,\n",
      "        ntk_alpha: float = 1.0,\n",
      "        ntk_beta: float = 32.0,\n",
      "        device: torch.device | None = None,\n",
      "    ) -> None:\n",
      "        super().__init__()\n",
      "        self.head_dim = head_dim\n",
      "        self.base = base\n",
      "        self.dtype = dtype\n",
      "        self.initial_context_length = initial_context_length\n",
      "        self.scaling_factor = scaling_factor\n",
      "        self.ntk_alpha = ntk_alpha\n",
      "        self.ntk_beta = ntk_beta\n",
      "        self.device = device\n",
      "---\n",
      "Metadata: \n",
      "{'source': 'tests/sample/model.py', 'name': 'RotaryEmbedding', 'type': 'class_definition', 'start_line': 63, 'docstring': None}\n",
      "\n",
      "****************************************\n",
      "\n",
      "Chunk no: 5\n",
      "Content: \n",
      "def _compute_concentration_and_inv_freq(self) -> torch.Tensor:\n",
      "        \"\"\"See YaRN paper: https://arxiv.org/abs/2309.00071\"\"\"\n",
      "        freq = self.base ** (\n",
      "            torch.arange(0, self.head_dim, 2, dtype=torch.float, device=self.device)\n",
      "            / self.head_dim\n",
      "        )\n",
      "        if self.scaling_factor > 1.0:\n",
      "            concentration = (\n",
      "                0.1 * math.log(self.scaling_factor) + 1.0\n",
      "            )  # YaRN concentration\n",
      "\n",
      "            d_half = self.head_dim / 2\n",
      "            # NTK by parts\n",
      "            low = (\n",
      "                d_half\n",
      "                * math.log(self.initial_context_length / (self.ntk_beta * 2 * math.pi))\n",
      "                / math.log(self.base)\n",
      "            )\n",
      "            high = (\n",
      "                d_half\n",
      "                * math.log(self.initial_context_length / (self.ntk_alpha * 2 * math.pi))\n",
      "                / math.log(self.base)\n",
      "            )\n",
      "            assert 0 < low < high < d_half - 1\n",
      "\n",
      "            interpolation = 1.0 / (self.scaling_factor * freq)\n",
      "            extrapolation = 1.0 / freq\n",
      "\n",
      "            ramp = (\n",
      "                torch.arange(d_half, dtype=torch.float32, device=freq.device) - low\n",
      "            ) / (high - low)\n",
      "            mask = 1 - ramp.clamp(0, 1)\n",
      "\n",
      "            inv_freq = interpolation * (1 - mask) + extrapolation * mask\n",
      "        else:\n",
      "            concentration = 1.0\n",
      "            inv_freq = 1.0 / freq\n",
      "\n",
      "        return concentration, inv_freq\n",
      "---\n",
      "Metadata: \n",
      "{'source': 'tests/sample/model.py', 'name': '_compute_concentration_and_inv_freq', 'type': 'method', 'parent_class': 'RotaryEmbedding', 'start_line': 85, 'docstring': 'See YaRN paper: https://arxiv.org/abs/2309.00071'}\n",
      "\n",
      "****************************************\n",
      "\n",
      "Chunk no: 6\n",
      "Content: \n",
      "def _compute_cos_sin(self, num_tokens: int):\n",
      "        concentration, inv_freq = self._compute_concentration_and_inv_freq()\n",
      "        t = torch.arange(num_tokens, dtype=torch.float32, device=self.device)\n",
      "        freqs = torch.einsum(\"i,j->ij\", t, inv_freq)\n",
      "        cos = freqs.cos() * concentration\n",
      "        sin = freqs.sin() * concentration\n",
      "        return cos, sin\n",
      "---\n",
      "Metadata: \n",
      "{'source': 'tests/sample/model.py', 'name': '_compute_cos_sin', 'type': 'method', 'parent_class': 'RotaryEmbedding', 'start_line': 125, 'docstring': None}\n",
      "\n",
      "****************************************\n",
      "\n",
      "Chunk no: 7\n",
      "Content: \n",
      "def forward(\n",
      "        self,\n",
      "        query: torch.Tensor,\n",
      "        key: torch.Tensor,\n",
      "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
      "        num_tokens = query.shape[0]\n",
      "        cos, sin = self._compute_cos_sin(num_tokens)\n",
      "\n",
      "        query_shape = query.shape\n",
      "        query = query.view(num_tokens, -1, self.head_dim)\n",
      "        query = _apply_rotary_emb(query, cos, sin)\n",
      "        query = query.reshape(query_shape)\n",
      "\n",
      "        key_shape = key.shape\n",
      "        key = key.view(num_tokens, -1, self.head_dim)\n",
      "        key = _apply_rotary_emb(key, cos, sin)\n",
      "        key = key.reshape(key_shape)\n",
      "        return query, key\n",
      "---\n",
      "Metadata: \n",
      "{'source': 'tests/sample/model.py', 'name': 'forward', 'type': 'method', 'parent_class': 'RotaryEmbedding', 'start_line': 133, 'docstring': None}\n",
      "\n",
      "****************************************\n",
      "\n",
      "Chunk no: 8\n",
      "Content: \n",
      "def sdpa(Q, K, V, S, sm_scale, sliding_window=0):\n",
      "    # sliding_window == 0 means no sliding window\n",
      "    n_tokens, n_heads, q_mult, d_head = Q.shape\n",
      "    assert K.shape == (n_tokens, n_heads, d_head)\n",
      "    assert V.shape == (n_tokens, n_heads, d_head)\n",
      "    K = K[:, :, None, :].expand(-1, -1, q_mult, -1)\n",
      "    V = V[:, :, None, :].expand(-1, -1, q_mult, -1)\n",
      "    S = S.reshape(n_heads, q_mult, 1, 1).expand(-1, -1, n_tokens, -1)\n",
      "    mask = torch.triu(Q.new_full((n_tokens, n_tokens), -float(\"inf\")), diagonal=1)\n",
      "    if sliding_window > 0:\n",
      "        mask += torch.tril(\n",
      "            mask.new_full((n_tokens, n_tokens), -float(\"inf\")), diagonal=-sliding_window\n",
      "        )\n",
      "    QK = torch.einsum(\"qhmd,khmd->hmqk\", Q, K)\n",
      "    QK *= sm_scale\n",
      "    QK += mask[None, None, :, :]\n",
      "    QK = torch.cat([QK, S], dim=-1)\n",
      "    W = torch.softmax(QK, dim=-1)\n",
      "    W = W[..., :-1]\n",
      "    attn = torch.einsum(\"hmqk,khmd->qhmd\", W, V)\n",
      "    return attn.reshape(n_tokens, -1)\n",
      "---\n",
      "Metadata: \n",
      "{'source': 'tests/sample/model.py', 'name': 'sdpa', 'type': 'function', 'parent_class': None, 'start_line': 153, 'docstring': None}\n",
      "\n",
      "****************************************\n",
      "\n",
      "Chunk no: 9\n",
      "Content: \n",
      "class AttentionBlock(torch.nn.Module):\n",
      "    def __init__(\n",
      "        self,\n",
      "        config: ModelConfig,\n",
      "        layer_idx: int = 0,\n",
      "        device: torch.device | None = None,\n",
      "    ):\n",
      "        super().__init__()\n",
      "        self.head_dim = config.head_dim\n",
      "        self.num_attention_heads = config.num_attention_heads\n",
      "        self.num_key_value_heads = config.num_key_value_heads\n",
      "        # Only apply sliding window to every other layer\n",
      "        self.sliding_window = config.sliding_window if layer_idx % 2 == 0 else 0\n",
      "        self.sinks = torch.nn.Parameter(\n",
      "            torch.empty(config.num_attention_heads, device=device, dtype=torch.bfloat16)\n",
      "        )\n",
      "        self.norm = RMSNorm(config.hidden_size, device=device)\n",
      "        qkv_dim = config.head_dim * (\n",
      "            config.num_attention_heads + 2 * config.num_key_value_heads\n",
      "        )\n",
      "        self.qkv = torch.nn.Linear(\n",
      "            config.hidden_size, qkv_dim, device=device, dtype=torch.bfloat16\n",
      "        )\n",
      "        self.out = torch.nn.Linear(\n",
      "            config.head_dim * config.num_attention_heads,\n",
      "            config.hidden_size,\n",
      "            device=device,\n",
      "            dtype=torch.bfloat16,\n",
      "        )\n",
      "        self.sm_scale = 1 / math.sqrt(config.head_dim)\n",
      "        self.rope = RotaryEmbedding(\n",
      "            config.head_dim,\n",
      "            config.rope_theta,\n",
      "            torch.float32,\n",
      "            initial_context_length=config.initial_context_length,\n",
      "            scaling_factor=config.rope_scaling_factor,\n",
      "            ntk_alpha=config.rope_ntk_alpha,\n",
      "---\n",
      "Metadata: \n",
      "{'source': 'tests/sample/model.py', 'name': 'AttentionBlock', 'type': 'class_definition', 'start_line': 176, 'docstring': None}\n",
      "\n",
      "****************************************\n",
      "\n",
      "Chunk no: 10\n",
      "Content: \n",
      "torch.float32,\n",
      "            initial_context_length=config.initial_context_length,\n",
      "            scaling_factor=config.rope_scaling_factor,\n",
      "            ntk_alpha=config.rope_ntk_alpha,\n",
      "            ntk_beta=config.rope_ntk_beta,\n",
      "            device=device,\n",
      "        )\n",
      "---\n",
      "Metadata: \n",
      "{'source': 'tests/sample/model.py', 'name': 'AttentionBlock', 'type': 'class_definition', 'start_line': 176, 'docstring': None}\n",
      "\n",
      "****************************************\n",
      "\n",
      "Chunk no: 11\n",
      "Content: \n",
      "def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
      "        t = self.norm(x)\n",
      "        qkv = self.qkv(t)\n",
      "        q = qkv[:, : self.num_attention_heads * self.head_dim].contiguous()\n",
      "        k = qkv[\n",
      "            :,\n",
      "            self.num_attention_heads\n",
      "            * self.head_dim : (self.num_attention_heads + self.num_key_value_heads)\n",
      "            * self.head_dim,\n",
      "        ].contiguous()\n",
      "        v = qkv[\n",
      "            :,\n",
      "            (self.num_attention_heads + self.num_key_value_heads)\n",
      "            * self.head_dim : (self.num_attention_heads + 2 * self.num_key_value_heads)\n",
      "            * self.head_dim,\n",
      "        ].contiguous()\n",
      "\n",
      "        q = q.view(\n",
      "            -1,\n",
      "            self.num_key_value_heads,\n",
      "            self.num_attention_heads // self.num_key_value_heads,\n",
      "            self.head_dim,\n",
      "        )\n",
      "        k = k.view(-1, self.num_key_value_heads, self.head_dim)\n",
      "        v = v.view(-1, self.num_key_value_heads, self.head_dim)\n",
      "        q, k = self.rope(q, k)\n",
      "        t = sdpa(q, k, v, self.sinks, self.sm_scale, self.sliding_window)\n",
      "        t = self.out(t)\n",
      "        t = x + t\n",
      "        return t\n",
      "---\n",
      "Metadata: \n",
      "{'source': 'tests/sample/model.py', 'name': 'forward', 'type': 'method', 'parent_class': 'AttentionBlock', 'start_line': 217, 'docstring': None}\n",
      "\n",
      "****************************************\n",
      "\n",
      "Chunk no: 12\n",
      "Content: \n",
      "def swiglu(x, alpha: float = 1.702, limit: float = 7.0):\n",
      "    x_glu, x_linear = x[..., ::2], x[..., 1::2]\n",
      "    # Clamp the input values\n",
      "    x_glu = x_glu.clamp(min=None, max=limit)\n",
      "    x_linear = x_linear.clamp(min=-limit, max=limit)\n",
      "    out_glu = x_glu * torch.sigmoid(alpha * x_glu)\n",
      "    # Note we add an extra bias of 1 to the linear layer\n",
      "    return out_glu * (x_linear + 1)\n",
      "---\n",
      "Metadata: \n",
      "{'source': 'tests/sample/model.py', 'name': 'swiglu', 'type': 'function', 'parent_class': None, 'start_line': 249, 'docstring': None}\n",
      "\n",
      "****************************************\n",
      "\n",
      "Chunk no: 13\n",
      "Content: \n",
      "class MLPBlock(torch.nn.Module):\n",
      "    def __init__(\n",
      "        self,\n",
      "        config: ModelConfig,\n",
      "        device: torch.device | None = None,\n",
      "    ):\n",
      "        super().__init__()\n",
      "        self.num_experts = config.num_experts\n",
      "        self.experts_per_token = config.experts_per_token\n",
      "        self.swiglu_limit = config.swiglu_limit\n",
      "        self.world_size = dist.get_world_size() if dist.is_initialized() else 1\n",
      "        self.norm = RMSNorm(config.hidden_size, device=device)\n",
      "        self.gate = torch.nn.Linear(\n",
      "            config.hidden_size, config.num_experts, device=device, dtype=torch.bfloat16\n",
      "        )\n",
      "        assert config.intermediate_size % self.world_size == 0\n",
      "        self.mlp1_weight = torch.nn.Parameter(\n",
      "            torch.empty(\n",
      "                (\n",
      "                    config.num_experts,\n",
      "                    config.intermediate_size * 2 // self.world_size,\n",
      "                    config.hidden_size,\n",
      "                ),\n",
      "                device=device,\n",
      "                dtype=torch.bfloat16,\n",
      "            )\n",
      "        )\n",
      "        self.mlp1_bias = torch.nn.Parameter(\n",
      "            torch.empty(\n",
      "                (config.num_experts, config.intermediate_size * 2 // self.world_size),\n",
      "                device=device,\n",
      "                dtype=torch.bfloat16,\n",
      "            )\n",
      "        )\n",
      "        self.mlp2_weight = torch.nn.Parameter(\n",
      "            torch.empty(\n",
      "                (\n",
      "                    config.num_experts,\n",
      "                    config.hidden_size,\n",
      "                    config.intermediate_size // self.world_size,\n",
      "---\n",
      "Metadata: \n",
      "{'source': 'tests/sample/model.py', 'name': 'MLPBlock', 'type': 'class_definition', 'start_line': 259, 'docstring': None}\n",
      "\n",
      "****************************************\n",
      "\n",
      "Chunk no: 14\n",
      "Content: \n",
      "torch.empty(\n",
      "                (\n",
      "                    config.num_experts,\n",
      "                    config.hidden_size,\n",
      "                    config.intermediate_size // self.world_size,\n",
      "                ),\n",
      "                device=device,\n",
      "                dtype=torch.bfloat16,\n",
      "            )\n",
      "        )\n",
      "        self.mlp2_bias = torch.nn.Parameter(\n",
      "            torch.empty(\n",
      "                (config.num_experts, config.hidden_size),\n",
      "                device=device,\n",
      "                dtype=torch.bfloat16,\n",
      "            )\n",
      "        )\n",
      "---\n",
      "Metadata: \n",
      "{'source': 'tests/sample/model.py', 'name': 'MLPBlock', 'type': 'class_definition', 'start_line': 259, 'docstring': None}\n",
      "\n",
      "****************************************\n",
      "\n",
      "Chunk no: 15\n",
      "Content: \n",
      "def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
      "        t = self.norm(x)\n",
      "        g = self.gate(t)\n",
      "        experts = torch.topk(g, k=self.experts_per_token, dim=-1, sorted=True)\n",
      "        expert_weights = torch.nn.functional.softmax(experts.values, dim=1)\n",
      "        expert_indices = experts.indices\n",
      "\n",
      "        # MLP #1\n",
      "        mlp1_weight = self.mlp1_weight[expert_indices, ...]\n",
      "        mlp1_bias = self.mlp1_bias[expert_indices, ...]\n",
      "        t = torch.einsum(\"beck,bk->bec\", mlp1_weight, t) + mlp1_bias\n",
      "        t = swiglu(t, limit=self.swiglu_limit)\n",
      "\n",
      "        # MLP #2\n",
      "        mlp2_weight = self.mlp2_weight[expert_indices, ...]\n",
      "        mlp2_bias = self.mlp2_bias[expert_indices, ...]\n",
      "        t = torch.einsum(\"beck,bek->bec\", mlp2_weight, t)\n",
      "        if self.world_size > 1:\n",
      "            dist.all_reduce(t, op=dist.ReduceOp.SUM)\n",
      "        t += mlp2_bias\n",
      "\n",
      "        # Weighted sum of experts\n",
      "        t = torch.einsum(\"bec,be->bc\", t, expert_weights)\n",
      "\n",
      "        return x + t\n",
      "---\n",
      "Metadata: \n",
      "{'source': 'tests/sample/model.py', 'name': 'forward', 'type': 'method', 'parent_class': 'MLPBlock', 'start_line': 312, 'docstring': None}\n",
      "\n",
      "****************************************\n",
      "\n",
      "Chunk no: 16\n",
      "Content: \n",
      "class TransformerBlock(torch.nn.Module):\n",
      "    def __init__(\n",
      "        self,\n",
      "        config: ModelConfig,\n",
      "        layer_idx: int,\n",
      "        device: torch.device | None = None,\n",
      "    ):\n",
      "        super().__init__()\n",
      "        self.layer_idx = layer_idx\n",
      "        self.attn = AttentionBlock(config, layer_idx, device)\n",
      "        self.mlp = MLPBlock(config, device)\n",
      "---\n",
      "Metadata: \n",
      "{'source': 'tests/sample/model.py', 'name': 'TransformerBlock', 'type': 'class_definition', 'start_line': 339, 'docstring': None}\n",
      "\n",
      "****************************************\n",
      "\n",
      "Chunk no: 17\n",
      "Content: \n",
      "def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
      "        x = self.attn(x)\n",
      "        x = self.mlp(x)\n",
      "        return x\n",
      "---\n",
      "Metadata: \n",
      "{'source': 'tests/sample/model.py', 'name': 'forward', 'type': 'method', 'parent_class': 'TransformerBlock', 'start_line': 351, 'docstring': None}\n",
      "\n",
      "****************************************\n",
      "\n",
      "Chunk no: 18\n",
      "Content: \n",
      "class Transformer(torch.nn.Module):\n",
      "    def __init__(\n",
      "        self,\n",
      "        config: ModelConfig,\n",
      "        device: torch.device | None = None,\n",
      "    ):\n",
      "        super().__init__()\n",
      "        self.embedding = torch.nn.Embedding(\n",
      "            config.vocab_size, config.hidden_size, device=device, dtype=torch.bfloat16\n",
      "        )\n",
      "        self.block = torch.nn.ModuleList(\n",
      "            [\n",
      "                TransformerBlock(config, layer_idx, device)\n",
      "                for layer_idx in range(config.num_hidden_layers)\n",
      "            ]\n",
      "        )\n",
      "        self.norm = RMSNorm(config.hidden_size, device=device)\n",
      "        self.unembedding = torch.nn.Linear(\n",
      "            config.hidden_size,\n",
      "            config.vocab_size,\n",
      "            bias=False,\n",
      "            device=device,\n",
      "            dtype=torch.bfloat16,\n",
      "        )\n",
      "---\n",
      "Metadata: \n",
      "{'source': 'tests/sample/model.py', 'name': 'Transformer', 'type': 'class_definition', 'start_line': 357, 'docstring': None}\n",
      "\n",
      "****************************************\n",
      "\n",
      "Chunk no: 19\n",
      "Content: \n",
      "def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
      "        x = self.embedding(x)\n",
      "        for block in self.block:\n",
      "            x = block(x)\n",
      "        x = self.norm(x)\n",
      "        x = self.unembedding(x)\n",
      "        return x\n",
      "---\n",
      "Metadata: \n",
      "{'source': 'tests/sample/model.py', 'name': 'forward', 'type': 'method', 'parent_class': 'Transformer', 'start_line': 382, 'docstring': None}\n",
      "\n",
      "****************************************\n",
      "\n",
      "Chunk no: 20\n",
      "Content: \n",
      "def from_checkpoint(\n",
      "        path: str, device: str | torch.device = \"cuda\"\n",
      "    ) -> \"Transformer\":\n",
      "        if not isinstance(device, torch.device):\n",
      "            device = torch.device(device)\n",
      "\n",
      "        config_path = os.path.join(path, \"config.json\")\n",
      "        with open(config_path, \"r\") as f:\n",
      "            json_config = json.load(f)\n",
      "            config = ModelConfig(**json_config)\n",
      "\n",
      "        model = Transformer(\n",
      "            config=config,\n",
      "            device=device,\n",
      "        )\n",
      "        model.eval()\n",
      "\n",
      "        # Load weights\n",
      "        my_rank = dist.get_rank() if dist.is_initialized() else 0\n",
      "        world_size = dist.get_world_size() if dist.is_initialized() else 1\n",
      "        per_rank_intermediate_size = config.intermediate_size // world_size\n",
      "\n",
      "        checkpoint = Checkpoint(path, device)\n",
      "\n",
      "        for name, param in model.named_parameters():\n",
      "            loaded_tensor = checkpoint.get(name)\n",
      "---\n",
      "Metadata: \n",
      "{'source': 'tests/sample/model.py', 'name': 'from_checkpoint', 'type': 'method', 'parent_class': 'Transformer', 'start_line': 391, 'docstring': None}\n",
      "\n",
      "****************************************\n",
      "\n",
      "Chunk no: 21\n",
      "Content: \n",
      "checkpoint = Checkpoint(path, device)\n",
      "\n",
      "        for name, param in model.named_parameters():\n",
      "            loaded_tensor = checkpoint.get(name)\n",
      "\n",
      "            # Note: it would be more efficient to do sharding before upcasting from MXFP4,\n",
      "            # but for simplicity we do it after.\n",
      "            if \"mlp1\" in name:  # both weight and bias\n",
      "                loaded_tensor = loaded_tensor[\n",
      "                    :,\n",
      "                    my_rank * 2\n",
      "                    * per_rank_intermediate_size : (my_rank + 1) * 2\n",
      "                    * per_rank_intermediate_size,\n",
      "                    ...,\n",
      "                ]\n",
      "            elif \"mlp2_weight\" in name:  # only weight\n",
      "                loaded_tensor = loaded_tensor[\n",
      "                    ...,\n",
      "                    my_rank\n",
      "                    * per_rank_intermediate_size : (my_rank + 1)\n",
      "                    * per_rank_intermediate_size,\n",
      "                ]\n",
      "            try:\n",
      "                param.data.copy_(loaded_tensor)\n",
      "            except:\n",
      "                print(f\"{name=} {param.data.shape=} {loaded_tensor.shape=}\")\n",
      "                raise\n",
      "\n",
      "        return model\n",
      "---\n",
      "Metadata: \n",
      "{'source': 'tests/sample/model.py', 'name': 'from_checkpoint', 'type': 'method', 'parent_class': 'Transformer', 'start_line': 391, 'docstring': None}\n",
      "\n",
      "****************************************\n",
      "\n",
      "Chunk no: 22\n",
      "Content: \n",
      "class TokenGenerator:\n",
      "    @torch.inference_mode()\n",
      "    def __init__(self, checkpoint: str, device: torch.device):\n",
      "        self.device = device\n",
      "        self.model = Transformer.from_checkpoint(checkpoint, device=self.device)\n",
      "\n",
      "    @torch.inference_mode()\n",
      "---\n",
      "Metadata: \n",
      "{'source': 'tests/sample/model.py', 'name': 'TokenGenerator', 'type': 'class_definition', 'start_line': 444, 'docstring': None}\n",
      "\n",
      "****************************************\n",
      "\n",
      "Chunk no: 23\n",
      "Content: \n",
      "def generate(self,\n",
      "                 prompt_tokens: list[int],\n",
      "                 stop_tokens: list[int],\n",
      "                 temperature: float = 1.0,\n",
      "                 max_tokens: int = 0,\n",
      "                 return_logprobs: bool = False):\n",
      "        tokens = list(prompt_tokens)\n",
      "        num_generated_tokens = 0\n",
      "        while max_tokens == 0 or num_generated_tokens < max_tokens:\n",
      "            logits = self.model(torch.as_tensor(tokens, dtype=torch.int32, device=self.device))[-1]\n",
      "            if temperature == 0.0:\n",
      "                predicted_token = torch.argmax(logits, dim=-1).item()\n",
      "            else:\n",
      "                probs = torch.softmax(logits * (1.0 / temperature), dim=-1)\n",
      "                predicted_token = torch.multinomial(probs, num_samples=1).item()\n",
      "            tokens.append(predicted_token)\n",
      "            num_generated_tokens += 1\n",
      "\n",
      "            if return_logprobs:\n",
      "                logprobs = torch.log_softmax(logits, dim=-1)\n",
      "                selected_logprobs = logprobs[predicted_token].item()\n",
      "                yield predicted_token, selected_logprobs\n",
      "            else:\n",
      "                yield predicted_token\n",
      "\n",
      "            if predicted_token in stop_tokens:\n",
      "                break\n",
      "---\n",
      "Metadata: \n",
      "{'source': 'tests/sample/model.py', 'name': 'generate', 'type': 'method', 'parent_class': 'TokenGenerator', 'start_line': 451, 'docstring': None}\n",
      "\n",
      "****************************************\n",
      "\n",
      "Chunk no: 24\n",
      "Content: \n",
      "def suppress_output(rank):\n",
      "    \"\"\"Suppress printing on the current device. Force printing with `force=True`.\"\"\"\n",
      "    import builtins as __builtin__\n",
      "    builtin_print = __builtin__.print\n",
      "\n",
      "    def print(*args, **kwargs):\n",
      "        force = kwargs.pop('force', False)\n",
      "        if force:\n",
      "            builtin_print(\"rank #%d:\" % rank, *args, **kwargs)\n",
      "        elif rank == 0:\n",
      "            builtin_print(*args, **kwargs)\n",
      "\n",
      "    __builtin__.print = print\n",
      "---\n",
      "Metadata: \n",
      "{'source': 'tests/sample/utils.py', 'name': 'suppress_output', 'type': 'function', 'parent_class': None, 'start_line': 6, 'docstring': 'Suppress printing on the current device. Force printing with `force=True`.'}\n",
      "\n",
      "****************************************\n",
      "\n",
      "Chunk no: 25\n",
      "Content: \n",
      "def init_distributed() -> torch.device:\n",
      "    \"\"\"Initialize the model for distributed inference.\"\"\"\n",
      "    # Initialize distributed inference\n",
      "    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
      "    rank = int(os.environ.get(\"RANK\", 0))\n",
      "    if world_size > 1:\n",
      "        dist.init_process_group(\n",
      "            backend=\"nccl\", init_method=\"env://\", world_size=world_size, rank=rank\n",
      "        )\n",
      "    torch.cuda.set_device(rank)\n",
      "    device = torch.device(f\"cuda:{rank}\")\n",
      "\n",
      "    # Warm up NCCL to avoid first-time latency\n",
      "    if world_size > 1:\n",
      "        x = torch.ones(1, device=device)\n",
      "        dist.all_reduce(x)\n",
      "        torch.cuda.synchronize(device)\n",
      "\n",
      "    suppress_output(rank)\n",
      "    return device\n",
      "---\n",
      "Metadata: \n",
      "{'source': 'tests/sample/utils.py', 'name': 'init_distributed', 'type': 'function', 'parent_class': None, 'start_line': 21, 'docstring': 'Initialize the model for distributed inference.'}\n",
      "\n",
      "****************************************\n",
      "\n",
      "Chunk no: 26\n",
      "Content: \n",
      "class Checkpoint:\n",
      "    def __init__(self, path: str, device: torch.device):\n",
      "        device_str = (\n",
      "            device.type\n",
      "            if device.index is None\n",
      "            else device.type + \":\" + str(device.index)\n",
      "        )\n",
      "        self.device_str = device_str\n",
      "\n",
      "        # Read from all files ending with .safetensors in the checkpoint directory\n",
      "        safetensor_files = [\n",
      "            os.path.join(path, fname)\n",
      "            for fname in os.listdir(path)\n",
      "            if fname.endswith(\".safetensors\")\n",
      "        ]\n",
      "        # Build a mapping from tensor name to (file, key)\n",
      "        tensor_name_to_file = {}\n",
      "        for safetensor_file in safetensor_files:\n",
      "            with safe_open(safetensor_file, framework=\"pt\", device=device_str) as f:\n",
      "                for key in f.keys():\n",
      "                    tensor_name_to_file[key] = safetensor_file\n",
      "\n",
      "        self.tensor_name_to_file = tensor_name_to_file\n",
      "---\n",
      "Metadata: \n",
      "{'source': 'tests/sample/weights.py', 'name': 'Checkpoint', 'type': 'class_definition', 'start_line': 28, 'docstring': None}\n",
      "\n",
      "****************************************\n",
      "\n",
      "Chunk no: 27\n",
      "Content: \n",
      "def get(self, name: str) -> torch.Tensor:\n",
      "        match PARAM_NAME_MAP.get(name, name):\n",
      "            case (blocks_name, scales_name):\n",
      "                # MoE weights: are in block-based MXFP4 format\n",
      "                return self._get_mxfp4_tensor(blocks_name, scales_name, dtype=torch.bfloat16)\n",
      "            case tensor_name:\n",
      "                # MoE biases and other weights\n",
      "                return self._get_tensor(tensor_name)\n",
      "---\n",
      "Metadata: \n",
      "{'source': 'tests/sample/weights.py', 'name': 'get', 'type': 'method', 'parent_class': 'Checkpoint', 'start_line': 52, 'docstring': None}\n",
      "\n",
      "****************************************\n",
      "\n",
      "Chunk no: 28\n",
      "Content: \n",
      "def _get_tensor(self, name: str) -> str:\n",
      "        assert name in self.tensor_name_to_file, f\"Tensor {name} not found in checkpoint.\"\n",
      "        with safe_open(\n",
      "            self.tensor_name_to_file[name], framework=\"pt\", device=self.device_str\n",
      "        ) as f:\n",
      "            return f.get_tensor(name)\n",
      "---\n",
      "Metadata: \n",
      "{'source': 'tests/sample/weights.py', 'name': '_get_tensor', 'type': 'method', 'parent_class': 'Checkpoint', 'start_line': 61, 'docstring': None}\n",
      "\n",
      "****************************************\n",
      "\n",
      "Chunk no: 29\n",
      "Content: \n",
      "def _get_mxfp4_tensor(\n",
      "        self,\n",
      "        blocks_name: str,\n",
      "        scales_name: str,\n",
      "        *,\n",
      "        dtype: torch.dtype = torch.bfloat16,\n",
      "        rows_per_chunk: int = 16384 * 512,\n",
      "    ) -> torch.Tensor:\n",
      "        assert blocks_name in self.tensor_name_to_file, (\n",
      "            f\"Blocks tensor {blocks_name} not found in checkpoint.\"\n",
      "        )\n",
      "        assert scales_name in self.tensor_name_to_file, (\n",
      "            f\"Scales tensor {scales_name} not found in checkpoint.\"\n",
      "        )\n",
      "\n",
      "        blocks = self._get_tensor(blocks_name)\n",
      "        scales = self._get_tensor(scales_name).to(torch.int32) - 127\n",
      "\n",
      "        assert blocks.shape[:-1] == scales.shape, (\n",
      "            f\"{blocks.shape=} does not match {scales.shape=}\"\n",
      "        )\n",
      "\n",
      "        lut = torch.tensor(FP4_VALUES, dtype=dtype, device=blocks.device)\n",
      "\n",
      "        *prefix_shape, G, B = blocks.shape\n",
      "        rows_total   = math.prod(prefix_shape) * G\n",
      "\n",
      "        blocks = blocks.reshape(rows_total, B)\n",
      "        scales = scales.reshape(rows_total, 1)\n",
      "\n",
      "        out = torch.empty(rows_total, B * 2, dtype=dtype, device=blocks.device)\n",
      "\n",
      "        for r0 in range(0, rows_total, rows_per_chunk):\n",
      "            r1 = min(r0 + rows_per_chunk, rows_total)\n",
      "\n",
      "            blk = blocks[r0:r1]\n",
      "            exp = scales[r0:r1]\n",
      "\n",
      "            # nibble indices -> int64\n",
      "            idx_lo = (blk & 0x0F).to(torch.long)\n",
      "            idx_hi = (blk >> 4).to(torch.long)\n",
      "\n",
      "            sub = out[r0:r1]\n",
      "            sub[:, 0::2] = lut[idx_lo]\n",
      "            sub[:, 1::2] = lut[idx_hi]\n",
      "---\n",
      "Metadata: \n",
      "{'source': 'tests/sample/weights.py', 'name': '_get_mxfp4_tensor', 'type': 'method', 'parent_class': 'Checkpoint', 'start_line': 68, 'docstring': None}\n",
      "\n",
      "****************************************\n",
      "\n",
      "Chunk no: 30\n",
      "Content: \n",
      "sub = out[r0:r1]\n",
      "            sub[:, 0::2] = lut[idx_lo]\n",
      "            sub[:, 1::2] = lut[idx_hi]\n",
      "\n",
      "            torch.ldexp(sub, exp, out=sub)\n",
      "            del idx_lo, idx_hi, blk, exp\n",
      "\n",
      "        return out.reshape(*prefix_shape, G, B * 2).view(*prefix_shape, G * B * 2)\n",
      "---\n",
      "Metadata: \n",
      "{'source': 'tests/sample/weights.py', 'name': '_get_mxfp4_tensor', 'type': 'method', 'parent_class': 'Checkpoint', 'start_line': 68, 'docstring': None}\n",
      "\n",
      "****************************************\n",
      "\n",
      "Chunk no: 31\n",
      "Content: \n",
      "def _get_mxfp4_tensor_copy(self, blocks_name: str, scales_name: str, dtype: torch.dtype = torch.bfloat16):\n",
      "        \"short version that uses a lot of memory\"\n",
      "\n",
      "        loaded_blocks = self._get_tensor(blocks_name)\n",
      "        # Split it into low and high nibbles, upcast to bytes, and interleave (for swiglu)\n",
      "        loaded_blocks_lo = loaded_blocks & 0x0F\n",
      "        loaded_blocks_hi = loaded_blocks >> 4\n",
      "        loaded_blocks = torch.stack((loaded_blocks_lo, loaded_blocks_hi), dim=-1)\n",
      "        loaded_blocks = loaded_blocks.view(*loaded_blocks.shape[:-2], loaded_blocks.shape[-2] * 2)\n",
      "\n",
      "        loaded_scales = self._get_tensor(scales_name)\n",
      "        # Upcast to int32 and subtract bias\n",
      "        loaded_scales = loaded_scales.int() - 127\n",
      "\n",
      "        # Convert MXFP4 numbers into target dtype\n",
      "        fp4_values = torch.tensor(FP4_VALUES, dtype=dtype, device=self.device_str)\n",
      "        loaded_tensor = torch.ldexp(fp4_values[loaded_blocks.int()], loaded_scales.unsqueeze(-1))\n",
      "        loaded_tensor = loaded_tensor.view(*loaded_tensor.shape[:-2], -1)\n",
      "        return loaded_tensor\n",
      "---\n",
      "Metadata: \n",
      "{'source': 'tests/sample/weights.py', 'name': '_get_mxfp4_tensor_copy', 'type': 'method', 'parent_class': 'Checkpoint', 'start_line': 119, 'docstring': 'short version that uses a lot of memory'}\n",
      "\n",
      "****************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "splitter = get_splitter()\n",
    "chunks = load_and_split_repository(\"tests/sample\")\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f'Chunk no: {i}')\n",
    "    print(f'Content: \\n{chunk.page_content}')\n",
    "    print('---')\n",
    "    print(f'Metadata: \\n{chunk.metadata}')\n",
    "    print('\\n****************************************\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
