{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa8c3faf-d752-4c30-a83a-37ea37dc6eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ingest import load_and_split_repository, get_splitter, split_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd1500a6-0061-4bf5-8778-a34e3629fad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_code = '''\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "from gpt_oss.torch.weights import Checkpoint\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    num_hidden_layers: int = 36\n",
    "    num_experts: int = 128\n",
    "    experts_per_token: int = 4\n",
    "    vocab_size: int = 201088\n",
    "    hidden_size: int = 2880\n",
    "    intermediate_size: int = 2880\n",
    "    swiglu_limit: float = 7.0\n",
    "    head_dim: int = 64\n",
    "    num_attention_heads: int = 64\n",
    "    num_key_value_heads: int = 8\n",
    "    sliding_window: int = 128\n",
    "    initial_context_length: int = 4096\n",
    "    rope_theta: float = 150000.0\n",
    "    rope_scaling_factor: float = 32.0\n",
    "    rope_ntk_alpha: float = 1.0\n",
    "    rope_ntk_beta: float = 32.0\n",
    "\n",
    "\n",
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, num_features: int, eps: float = 1e-05, device: torch.device | None = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.scale = torch.nn.Parameter(\n",
    "            torch.ones(num_features, device=device, dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        assert x.shape[-1] == self.num_features\n",
    "        t, dtype = x.float(), x.dtype\n",
    "        t = t * torch.rsqrt(torch.mean(t**2, dim=-1, keepdim=True) + self.eps)\n",
    "        return (t * self.scale).to(dtype)\n",
    "\n",
    "\n",
    "def _apply_rotary_emb(\n",
    "    x: torch.Tensor,\n",
    "    cos: torch.Tensor,\n",
    "    sin: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    cos = cos.unsqueeze(-2).to(x.dtype)\n",
    "    sin = sin.unsqueeze(-2).to(x.dtype)\n",
    "    x1, x2 = torch.chunk(x, 2, dim=-1)\n",
    "    o1 = x1 * cos - x2 * sin\n",
    "    o2 = x2 * cos + x1 * sin\n",
    "    return torch.cat((o1, o2), dim=-1)\n",
    "\n",
    "\n",
    "class RotaryEmbedding(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        head_dim: int,\n",
    "        base: int,\n",
    "        dtype: torch.dtype,\n",
    "        initial_context_length: int = 4096,\n",
    "        scaling_factor: float = 1.0,\n",
    "        ntk_alpha: float = 1.0,\n",
    "        ntk_beta: float = 32.0,\n",
    "        device: torch.device | None = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.base = base\n",
    "        self.dtype = dtype\n",
    "        self.initial_context_length = initial_context_length\n",
    "        self.scaling_factor = scaling_factor\n",
    "        self.ntk_alpha = ntk_alpha\n",
    "        self.ntk_beta = ntk_beta\n",
    "        self.device = device\n",
    "\n",
    "    def _compute_concentration_and_inv_freq(self) -> torch.Tensor:\n",
    "        \"\"\"See YaRN paper: https://arxiv.org/abs/2309.00071\"\"\"\n",
    "        freq = self.base ** (\n",
    "            torch.arange(0, self.head_dim, 2, dtype=torch.float, device=self.device)\n",
    "            / self.head_dim\n",
    "        )\n",
    "        if self.scaling_factor > 1.0:\n",
    "            concentration = (\n",
    "                0.1 * math.log(self.scaling_factor) + 1.0\n",
    "            )  # YaRN concentration\n",
    "\n",
    "            d_half = self.head_dim / 2\n",
    "            # NTK by parts\n",
    "            low = (\n",
    "                d_half\n",
    "                * math.log(self.initial_context_length / (self.ntk_beta * 2 * math.pi))\n",
    "                / math.log(self.base)\n",
    "            )\n",
    "            high = (\n",
    "                d_half\n",
    "                * math.log(self.initial_context_length / (self.ntk_alpha * 2 * math.pi))\n",
    "                / math.log(self.base)\n",
    "            )\n",
    "            assert 0 < low < high < d_half - 1\n",
    "\n",
    "            interpolation = 1.0 / (self.scaling_factor * freq)\n",
    "            extrapolation = 1.0 / freq\n",
    "\n",
    "            ramp = (\n",
    "                torch.arange(d_half, dtype=torch.float32, device=freq.device) - low\n",
    "            ) / (high - low)\n",
    "            mask = 1 - ramp.clamp(0, 1)\n",
    "\n",
    "            inv_freq = interpolation * (1 - mask) + extrapolation * mask\n",
    "        else:\n",
    "            concentration = 1.0\n",
    "            inv_freq = 1.0 / freq\n",
    "\n",
    "        return concentration, inv_freq\n",
    "\n",
    "    def _compute_cos_sin(self, num_tokens: int):\n",
    "        concentration, inv_freq = self._compute_concentration_and_inv_freq()\n",
    "        t = torch.arange(num_tokens, dtype=torch.float32, device=self.device)\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, inv_freq)\n",
    "        cos = freqs.cos() * concentration\n",
    "        sin = freqs.sin() * concentration\n",
    "        return cos, sin\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        num_tokens = query.shape[0]\n",
    "        cos, sin = self._compute_cos_sin(num_tokens)\n",
    "\n",
    "        query_shape = query.shape\n",
    "        query = query.view(num_tokens, -1, self.head_dim)\n",
    "        query = _apply_rotary_emb(query, cos, sin)\n",
    "        query = query.reshape(query_shape)\n",
    "\n",
    "        key_shape = key.shape\n",
    "        key = key.view(num_tokens, -1, self.head_dim)\n",
    "        key = _apply_rotary_emb(key, cos, sin)\n",
    "        key = key.reshape(key_shape)\n",
    "        return query, key\n",
    "\n",
    "\n",
    "def sdpa(Q, K, V, S, sm_scale, sliding_window=0):\n",
    "    # sliding_window == 0 means no sliding window\n",
    "    n_tokens, n_heads, q_mult, d_head = Q.shape\n",
    "    assert K.shape == (n_tokens, n_heads, d_head)\n",
    "    assert V.shape == (n_tokens, n_heads, d_head)\n",
    "    K = K[:, :, None, :].expand(-1, -1, q_mult, -1)\n",
    "    V = V[:, :, None, :].expand(-1, -1, q_mult, -1)\n",
    "    S = S.reshape(n_heads, q_mult, 1, 1).expand(-1, -1, n_tokens, -1)\n",
    "    mask = torch.triu(Q.new_full((n_tokens, n_tokens), -float(\"inf\")), diagonal=1)\n",
    "    if sliding_window > 0:\n",
    "        mask += torch.tril(\n",
    "            mask.new_full((n_tokens, n_tokens), -float(\"inf\")), diagonal=-sliding_window\n",
    "        )\n",
    "    QK = torch.einsum(\"qhmd,khmd->hmqk\", Q, K)\n",
    "    QK *= sm_scale\n",
    "    QK += mask[None, None, :, :]\n",
    "    QK = torch.cat([QK, S], dim=-1)\n",
    "    W = torch.softmax(QK, dim=-1)\n",
    "    W = W[..., :-1]\n",
    "    attn = torch.einsum(\"hmqk,khmd->qhmd\", W, V)\n",
    "    return attn.reshape(n_tokens, -1)\n",
    "\n",
    "\n",
    "class AttentionBlock(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: ModelConfig,\n",
    "        layer_idx: int = 0,\n",
    "        device: torch.device | None = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.head_dim = config.head_dim\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.num_key_value_heads = config.num_key_value_heads\n",
    "        # Only apply sliding window to every other layer\n",
    "        self.sliding_window = config.sliding_window if layer_idx % 2 == 0 else 0\n",
    "        self.sinks = torch.nn.Parameter(\n",
    "            torch.empty(config.num_attention_heads, device=device, dtype=torch.bfloat16)\n",
    "        )\n",
    "        self.norm = RMSNorm(config.hidden_size, device=device)\n",
    "        qkv_dim = config.head_dim * (\n",
    "            config.num_attention_heads + 2 * config.num_key_value_heads\n",
    "        )\n",
    "        self.qkv = torch.nn.Linear(\n",
    "            config.hidden_size, qkv_dim, device=device, dtype=torch.bfloat16\n",
    "        )\n",
    "        self.out = torch.nn.Linear(\n",
    "            config.head_dim * config.num_attention_heads,\n",
    "            config.hidden_size,\n",
    "            device=device,\n",
    "            dtype=torch.bfloat16,\n",
    "        )\n",
    "        self.sm_scale = 1 / math.sqrt(config.head_dim)\n",
    "        self.rope = RotaryEmbedding(\n",
    "            config.head_dim,\n",
    "            config.rope_theta,\n",
    "            torch.float32,\n",
    "            initial_context_length=config.initial_context_length,\n",
    "            scaling_factor=config.rope_scaling_factor,\n",
    "            ntk_alpha=config.rope_ntk_alpha,\n",
    "            ntk_beta=config.rope_ntk_beta,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        t = self.norm(x)\n",
    "        qkv = self.qkv(t)\n",
    "        q = qkv[:, : self.num_attention_heads * self.head_dim].contiguous()\n",
    "        k = qkv[\n",
    "            :,\n",
    "            self.num_attention_heads\n",
    "            * self.head_dim : (self.num_attention_heads + self.num_key_value_heads)\n",
    "            * self.head_dim,\n",
    "        ].contiguous()\n",
    "        v = qkv[\n",
    "            :,\n",
    "            (self.num_attention_heads + self.num_key_value_heads)\n",
    "            * self.head_dim : (self.num_attention_heads + 2 * self.num_key_value_heads)\n",
    "            * self.head_dim,\n",
    "        ].contiguous()\n",
    "\n",
    "        q = q.view(\n",
    "            -1,\n",
    "            self.num_key_value_heads,\n",
    "            self.num_attention_heads // self.num_key_value_heads,\n",
    "            self.head_dim,\n",
    "        )\n",
    "        k = k.view(-1, self.num_key_value_heads, self.head_dim)\n",
    "        v = v.view(-1, self.num_key_value_heads, self.head_dim)\n",
    "        q, k = self.rope(q, k)\n",
    "        t = sdpa(q, k, v, self.sinks, self.sm_scale, self.sliding_window)\n",
    "        t = self.out(t)\n",
    "        t = x + t\n",
    "        return t\n",
    "\n",
    "\n",
    "def swiglu(x, alpha: float = 1.702, limit: float = 7.0):\n",
    "    x_glu, x_linear = x[..., ::2], x[..., 1::2]\n",
    "    # Clamp the input values\n",
    "    x_glu = x_glu.clamp(min=None, max=limit)\n",
    "    x_linear = x_linear.clamp(min=-limit, max=limit)\n",
    "    out_glu = x_glu * torch.sigmoid(alpha * x_glu)\n",
    "    # Note we add an extra bias of 1 to the linear layer\n",
    "    return out_glu * (x_linear + 1)\n",
    "\n",
    "\n",
    "class MLPBlock(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: ModelConfig,\n",
    "        device: torch.device | None = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_experts = config.num_experts\n",
    "        self.experts_per_token = config.experts_per_token\n",
    "        self.swiglu_limit = config.swiglu_limit\n",
    "        self.world_size = dist.get_world_size() if dist.is_initialized() else 1\n",
    "        self.norm = RMSNorm(config.hidden_size, device=device)\n",
    "        self.gate = torch.nn.Linear(\n",
    "            config.hidden_size, config.num_experts, device=device, dtype=torch.bfloat16\n",
    "        )\n",
    "        assert config.intermediate_size % self.world_size == 0\n",
    "        self.mlp1_weight = torch.nn.Parameter(\n",
    "            torch.empty(\n",
    "                (\n",
    "                    config.num_experts,\n",
    "                    config.intermediate_size * 2 // self.world_size,\n",
    "                    config.hidden_size,\n",
    "                ),\n",
    "                device=device,\n",
    "                dtype=torch.bfloat16,\n",
    "            )\n",
    "        )\n",
    "        self.mlp1_bias = torch.nn.Parameter(\n",
    "            torch.empty(\n",
    "                (config.num_experts, config.intermediate_size * 2 // self.world_size),\n",
    "                device=device,\n",
    "                dtype=torch.bfloat16,\n",
    "            )\n",
    "        )\n",
    "        self.mlp2_weight = torch.nn.Parameter(\n",
    "            torch.empty(\n",
    "                (\n",
    "                    config.num_experts,\n",
    "                    config.hidden_size,\n",
    "                    config.intermediate_size // self.world_size,\n",
    "                ),\n",
    "                device=device,\n",
    "                dtype=torch.bfloat16,\n",
    "            )\n",
    "        )\n",
    "        self.mlp2_bias = torch.nn.Parameter(\n",
    "            torch.empty(\n",
    "                (config.num_experts, config.hidden_size),\n",
    "                device=device,\n",
    "                dtype=torch.bfloat16,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        t = self.norm(x)\n",
    "        g = self.gate(t)\n",
    "        experts = torch.topk(g, k=self.experts_per_token, dim=-1, sorted=True)\n",
    "        expert_weights = torch.nn.functional.softmax(experts.values, dim=1)\n",
    "        expert_indices = experts.indices\n",
    "\n",
    "        # MLP #1\n",
    "        mlp1_weight = self.mlp1_weight[expert_indices, ...]\n",
    "        mlp1_bias = self.mlp1_bias[expert_indices, ...]\n",
    "        t = torch.einsum(\"beck,bk->bec\", mlp1_weight, t) + mlp1_bias\n",
    "        t = swiglu(t, limit=self.swiglu_limit)\n",
    "\n",
    "        # MLP #2\n",
    "        mlp2_weight = self.mlp2_weight[expert_indices, ...]\n",
    "        mlp2_bias = self.mlp2_bias[expert_indices, ...]\n",
    "        t = torch.einsum(\"beck,bek->bec\", mlp2_weight, t)\n",
    "        if self.world_size > 1:\n",
    "            dist.all_reduce(t, op=dist.ReduceOp.SUM)\n",
    "        t += mlp2_bias\n",
    "\n",
    "        # Weighted sum of experts\n",
    "        t = torch.einsum(\"bec,be->bc\", t, expert_weights)\n",
    "\n",
    "        return x + t\n",
    "\n",
    "\n",
    "class TransformerBlock(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: ModelConfig,\n",
    "        layer_idx: int,\n",
    "        device: torch.device | None = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layer_idx = layer_idx\n",
    "        self.attn = AttentionBlock(config, layer_idx, device)\n",
    "        self.mlp = MLPBlock(config, device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.attn(x)\n",
    "        x = self.mlp(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: ModelConfig,\n",
    "        device: torch.device | None = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(\n",
    "            config.vocab_size, config.hidden_size, device=device, dtype=torch.bfloat16\n",
    "        )\n",
    "        self.block = torch.nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(config, layer_idx, device)\n",
    "                for layer_idx in range(config.num_hidden_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.norm = RMSNorm(config.hidden_size, device=device)\n",
    "        self.unembedding = torch.nn.Linear(\n",
    "            config.hidden_size,\n",
    "            config.vocab_size,\n",
    "            bias=False,\n",
    "            device=device,\n",
    "            dtype=torch.bfloat16,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.embedding(x)\n",
    "        for block in self.block:\n",
    "            x = block(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.unembedding(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def from_checkpoint(\n",
    "        path: str, device: str | torch.device = \"cuda\"\n",
    "    ) -> \"Transformer\":\n",
    "        if not isinstance(device, torch.device):\n",
    "            device = torch.device(device)\n",
    "\n",
    "        config_path = os.path.join(path, \"config.json\")\n",
    "        with open(config_path, \"r\") as f:\n",
    "            json_config = json.load(f)\n",
    "            config = ModelConfig(**json_config)\n",
    "\n",
    "        model = Transformer(\n",
    "            config=config,\n",
    "            device=device,\n",
    "        )\n",
    "        model.eval()\n",
    "\n",
    "        # Load weights\n",
    "        my_rank = dist.get_rank() if dist.is_initialized() else 0\n",
    "        world_size = dist.get_world_size() if dist.is_initialized() else 1\n",
    "        per_rank_intermediate_size = config.intermediate_size // world_size\n",
    "\n",
    "        checkpoint = Checkpoint(path, device)\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            loaded_tensor = checkpoint.get(name)\n",
    "\n",
    "            # Note: it would be more efficient to do sharding before upcasting from MXFP4,\n",
    "            # but for simplicity we do it after.\n",
    "            if \"mlp1\" in name:  # both weight and bias\n",
    "                loaded_tensor = loaded_tensor[\n",
    "                    :,\n",
    "                    my_rank * 2\n",
    "                    * per_rank_intermediate_size : (my_rank + 1) * 2\n",
    "                    * per_rank_intermediate_size,\n",
    "                    ...,\n",
    "                ]\n",
    "            elif \"mlp2_weight\" in name:  # only weight\n",
    "                loaded_tensor = loaded_tensor[\n",
    "                    ...,\n",
    "                    my_rank\n",
    "                    * per_rank_intermediate_size : (my_rank + 1)\n",
    "                    * per_rank_intermediate_size,\n",
    "                ]\n",
    "            try:\n",
    "                param.data.copy_(loaded_tensor)\n",
    "            except:\n",
    "                print(f\"{name=} {param.data.shape=} {loaded_tensor.shape=}\")\n",
    "                raise\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "class TokenGenerator:\n",
    "    @torch.inference_mode()\n",
    "    def __init__(self, checkpoint: str, device: torch.device):\n",
    "        self.device = device\n",
    "        self.model = Transformer.from_checkpoint(checkpoint, device=self.device)\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def generate(self,\n",
    "                 prompt_tokens: list[int],\n",
    "                 stop_tokens: list[int],\n",
    "                 temperature: float = 1.0,\n",
    "                 max_tokens: int = 0,\n",
    "                 return_logprobs: bool = False):\n",
    "        tokens = list(prompt_tokens)\n",
    "        num_generated_tokens = 0\n",
    "        while max_tokens == 0 or num_generated_tokens < max_tokens:\n",
    "            logits = self.model(torch.as_tensor(tokens, dtype=torch.int32, device=self.device))[-1]\n",
    "            if temperature == 0.0:\n",
    "                predicted_token = torch.argmax(logits, dim=-1).item()\n",
    "            else:\n",
    "                probs = torch.softmax(logits * (1.0 / temperature), dim=-1)\n",
    "                predicted_token = torch.multinomial(probs, num_samples=1).item()\n",
    "            tokens.append(predicted_token)\n",
    "            num_generated_tokens += 1\n",
    "\n",
    "            if return_logprobs:\n",
    "                logprobs = torch.log_softmax(logits, dim=-1)\n",
    "                selected_logprobs = logprobs[predicted_token].item()\n",
    "                yield predicted_token, selected_logprobs\n",
    "            else:\n",
    "                yield predicted_token\n",
    "\n",
    "            if predicted_token in stop_tokens:\n",
    "                break\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb63428a-cc16-42ec-8f62-cb61716fa22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Content: \n",
      "import json\n",
      "import math\n",
      "import os\n",
      "from dataclasses import dataclass\n",
      "\n",
      "import torch\n",
      "import torch.distributed as dist\n",
      "\n",
      "from gpt_oss.torch.weights import Checkpoint\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class ModelConfig:\n",
      "    num_hidden_layers: int = 36\n",
      "    num_experts: int = 128\n",
      "    experts_per_token: int = 4\n",
      "    vocab_size: int = 201088\n",
      "    hidden_size: int = 2880\n",
      "    intermediate_size: int = 2880\n",
      "    swiglu_limit: float = 7.0\n",
      "    head_dim: int = 64\n",
      "    num_attention_heads: int = 64\n",
      "    num_key_value_heads: int = 8\n",
      "    sliding_window: int = 128\n",
      "    initial_context_length: int = 4096\n",
      "    rope_theta: float = 150000.0\n",
      "    rope_scaling_factor: float = 32.0\n",
      "    rope_ntk_alpha: float = 1.0\n",
      "    rope_ntk_beta: float = 32.0\n",
      "-----\n",
      "Metadata: \n",
      "{}\n",
      "\n",
      "--------------------------\n",
      "\n",
      "1\n",
      "Content: \n",
      "class RMSNorm(torch.nn.Module):\n",
      "    def __init__(\n",
      "        self, num_features: int, eps: float = 1e-05, device: torch.device | None = None\n",
      "    ):\n",
      "        super().__init__()\n",
      "        self.num_features = num_features\n",
      "        self.eps = eps\n",
      "        self.scale = torch.nn.Parameter(\n",
      "            torch.ones(num_features, device=device, dtype=torch.float32)\n",
      "        )\n",
      "\n",
      "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
      "        assert x.shape[-1] == self.num_features\n",
      "        t, dtype = x.float(), x.dtype\n",
      "        t = t * torch.rsqrt(torch.mean(t**2, dim=-1, keepdim=True) + self.eps)\n",
      "        return (t * self.scale).to(dtype)\n",
      "\n",
      "\n",
      "def _apply_rotary_emb(\n",
      "    x: torch.Tensor,\n",
      "    cos: torch.Tensor,\n",
      "    sin: torch.Tensor,\n",
      ") -> torch.Tensor:\n",
      "    cos = cos.unsqueeze(-2).to(x.dtype)\n",
      "    sin = sin.unsqueeze(-2).to(x.dtype)\n",
      "    x1, x2 = torch.chunk(x, 2, dim=-1)\n",
      "    o1 = x1 * cos - x2 * sin\n",
      "    o2 = x2 * cos + x1 * sin\n",
      "    return torch.cat((o1, o2), dim=-1)\n",
      "-----\n",
      "Metadata: \n",
      "{}\n",
      "\n",
      "--------------------------\n",
      "\n",
      "2\n",
      "Content: \n",
      "class RotaryEmbedding(torch.nn.Module):\n",
      "    def __init__(\n",
      "        self,\n",
      "        head_dim: int,\n",
      "        base: int,\n",
      "        dtype: torch.dtype,\n",
      "        initial_context_length: int = 4096,\n",
      "        scaling_factor: float = 1.0,\n",
      "        ntk_alpha: float = 1.0,\n",
      "        ntk_beta: float = 32.0,\n",
      "        device: torch.device | None = None,\n",
      "    ) -> None:\n",
      "        super().__init__()\n",
      "        self.head_dim = head_dim\n",
      "        self.base = base\n",
      "        self.dtype = dtype\n",
      "        self.initial_context_length = initial_context_length\n",
      "        self.scaling_factor = scaling_factor\n",
      "        self.ntk_alpha = ntk_alpha\n",
      "        self.ntk_beta = ntk_beta\n",
      "        self.device = device\n",
      "\n",
      "    def _compute_concentration_and_inv_freq(self) -> torch.Tensor:\n",
      "        \"\"\"See YaRN paper: https://arxiv.org/abs/2309.00071\"\"\"\n",
      "        freq = self.base ** (\n",
      "            torch.arange(0, self.head_dim, 2, dtype=torch.float, device=self.device)\n",
      "            / self.head_dim\n",
      "        )\n",
      "        if self.scaling_factor > 1.0:\n",
      "            concentration = (\n",
      "                0.1 * math.log(self.scaling_factor) + 1.0\n",
      "            )  # YaRN concentration\n",
      "-----\n",
      "Metadata: \n",
      "{}\n",
      "\n",
      "--------------------------\n",
      "\n",
      "3\n",
      "Content: \n",
      "d_half = self.head_dim / 2\n",
      "            # NTK by parts\n",
      "            low = (\n",
      "                d_half\n",
      "                * math.log(self.initial_context_length / (self.ntk_beta * 2 * math.pi))\n",
      "                / math.log(self.base)\n",
      "            )\n",
      "            high = (\n",
      "                d_half\n",
      "                * math.log(self.initial_context_length / (self.ntk_alpha * 2 * math.pi))\n",
      "                / math.log(self.base)\n",
      "            )\n",
      "            assert 0 < low < high < d_half - 1\n",
      "\n",
      "            interpolation = 1.0 / (self.scaling_factor * freq)\n",
      "            extrapolation = 1.0 / freq\n",
      "\n",
      "            ramp = (\n",
      "                torch.arange(d_half, dtype=torch.float32, device=freq.device) - low\n",
      "            ) / (high - low)\n",
      "            mask = 1 - ramp.clamp(0, 1)\n",
      "\n",
      "            inv_freq = interpolation * (1 - mask) + extrapolation * mask\n",
      "        else:\n",
      "            concentration = 1.0\n",
      "            inv_freq = 1.0 / freq\n",
      "\n",
      "        return concentration, inv_freq\n",
      "\n",
      "    def _compute_cos_sin(self, num_tokens: int):\n",
      "        concentration, inv_freq = self._compute_concentration_and_inv_freq()\n",
      "        t = torch.arange(num_tokens, dtype=torch.float32, device=self.device)\n",
      "        freqs = torch.einsum(\"i,j->ij\", t, inv_freq)\n",
      "        cos = freqs.cos() * concentration\n",
      "        sin = freqs.sin() * concentration\n",
      "        return cos, sin\n",
      "-----\n",
      "Metadata: \n",
      "{}\n",
      "\n",
      "--------------------------\n",
      "\n",
      "4\n",
      "Content: \n",
      "def forward(\n",
      "        self,\n",
      "        query: torch.Tensor,\n",
      "        key: torch.Tensor,\n",
      "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
      "        num_tokens = query.shape[0]\n",
      "        cos, sin = self._compute_cos_sin(num_tokens)\n",
      "\n",
      "        query_shape = query.shape\n",
      "        query = query.view(num_tokens, -1, self.head_dim)\n",
      "        query = _apply_rotary_emb(query, cos, sin)\n",
      "        query = query.reshape(query_shape)\n",
      "\n",
      "        key_shape = key.shape\n",
      "        key = key.view(num_tokens, -1, self.head_dim)\n",
      "        key = _apply_rotary_emb(key, cos, sin)\n",
      "        key = key.reshape(key_shape)\n",
      "        return query, key\n",
      "-----\n",
      "Metadata: \n",
      "{}\n",
      "\n",
      "--------------------------\n",
      "\n",
      "5\n",
      "Content: \n",
      "def sdpa(Q, K, V, S, sm_scale, sliding_window=0):\n",
      "    # sliding_window == 0 means no sliding window\n",
      "    n_tokens, n_heads, q_mult, d_head = Q.shape\n",
      "    assert K.shape == (n_tokens, n_heads, d_head)\n",
      "    assert V.shape == (n_tokens, n_heads, d_head)\n",
      "    K = K[:, :, None, :].expand(-1, -1, q_mult, -1)\n",
      "    V = V[:, :, None, :].expand(-1, -1, q_mult, -1)\n",
      "    S = S.reshape(n_heads, q_mult, 1, 1).expand(-1, -1, n_tokens, -1)\n",
      "    mask = torch.triu(Q.new_full((n_tokens, n_tokens), -float(\"inf\")), diagonal=1)\n",
      "    if sliding_window > 0:\n",
      "        mask += torch.tril(\n",
      "            mask.new_full((n_tokens, n_tokens), -float(\"inf\")), diagonal=-sliding_window\n",
      "        )\n",
      "    QK = torch.einsum(\"qhmd,khmd->hmqk\", Q, K)\n",
      "    QK *= sm_scale\n",
      "    QK += mask[None, None, :, :]\n",
      "    QK = torch.cat([QK, S], dim=-1)\n",
      "    W = torch.softmax(QK, dim=-1)\n",
      "    W = W[..., :-1]\n",
      "    attn = torch.einsum(\"hmqk,khmd->qhmd\", W, V)\n",
      "    return attn.reshape(n_tokens, -1)\n",
      "-----\n",
      "Metadata: \n",
      "{}\n",
      "\n",
      "--------------------------\n",
      "\n",
      "6\n",
      "Content: \n",
      "class AttentionBlock(torch.nn.Module):\n",
      "    def __init__(\n",
      "        self,\n",
      "        config: ModelConfig,\n",
      "        layer_idx: int = 0,\n",
      "        device: torch.device | None = None,\n",
      "    ):\n",
      "        super().__init__()\n",
      "        self.head_dim = config.head_dim\n",
      "        self.num_attention_heads = config.num_attention_heads\n",
      "        self.num_key_value_heads = config.num_key_value_heads\n",
      "        # Only apply sliding window to every other layer\n",
      "        self.sliding_window = config.sliding_window if layer_idx % 2 == 0 else 0\n",
      "        self.sinks = torch.nn.Parameter(\n",
      "            torch.empty(config.num_attention_heads, device=device, dtype=torch.bfloat16)\n",
      "        )\n",
      "        self.norm = RMSNorm(config.hidden_size, device=device)\n",
      "        qkv_dim = config.head_dim * (\n",
      "            config.num_attention_heads + 2 * config.num_key_value_heads\n",
      "        )\n",
      "        self.qkv = torch.nn.Linear(\n",
      "            config.hidden_size, qkv_dim, device=device, dtype=torch.bfloat16\n",
      "        )\n",
      "        self.out = torch.nn.Linear(\n",
      "            config.head_dim * config.num_attention_heads,\n",
      "            config.hidden_size,\n",
      "            device=device,\n",
      "            dtype=torch.bfloat16,\n",
      "        )\n",
      "        self.sm_scale = 1 / math.sqrt(config.head_dim)\n",
      "        self.rope = RotaryEmbedding(\n",
      "            config.head_dim,\n",
      "            config.rope_theta,\n",
      "            torch.float32,\n",
      "            initial_context_length=config.initial_context_length,\n",
      "            scaling_factor=config.rope_scaling_factor,\n",
      "            ntk_alpha=config.rope_ntk_alpha,\n",
      "-----\n",
      "Metadata: \n",
      "{}\n",
      "\n",
      "--------------------------\n",
      "\n",
      "7\n",
      "Content: \n",
      "self.rope = RotaryEmbedding(\n",
      "            config.head_dim,\n",
      "            config.rope_theta,\n",
      "            torch.float32,\n",
      "            initial_context_length=config.initial_context_length,\n",
      "            scaling_factor=config.rope_scaling_factor,\n",
      "            ntk_alpha=config.rope_ntk_alpha,\n",
      "            ntk_beta=config.rope_ntk_beta,\n",
      "            device=device,\n",
      "        )\n",
      "-----\n",
      "Metadata: \n",
      "{}\n",
      "\n",
      "--------------------------\n",
      "\n",
      "8\n",
      "Content: \n",
      "def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
      "        t = self.norm(x)\n",
      "        qkv = self.qkv(t)\n",
      "        q = qkv[:, : self.num_attention_heads * self.head_dim].contiguous()\n",
      "        k = qkv[\n",
      "            :,\n",
      "            self.num_attention_heads\n",
      "            * self.head_dim : (self.num_attention_heads + self.num_key_value_heads)\n",
      "            * self.head_dim,\n",
      "        ].contiguous()\n",
      "        v = qkv[\n",
      "            :,\n",
      "            (self.num_attention_heads + self.num_key_value_heads)\n",
      "            * self.head_dim : (self.num_attention_heads + 2 * self.num_key_value_heads)\n",
      "            * self.head_dim,\n",
      "        ].contiguous()\n",
      "\n",
      "        q = q.view(\n",
      "            -1,\n",
      "            self.num_key_value_heads,\n",
      "            self.num_attention_heads // self.num_key_value_heads,\n",
      "            self.head_dim,\n",
      "        )\n",
      "        k = k.view(-1, self.num_key_value_heads, self.head_dim)\n",
      "        v = v.view(-1, self.num_key_value_heads, self.head_dim)\n",
      "        q, k = self.rope(q, k)\n",
      "        t = sdpa(q, k, v, self.sinks, self.sm_scale, self.sliding_window)\n",
      "        t = self.out(t)\n",
      "        t = x + t\n",
      "        return t\n",
      "-----\n",
      "Metadata: \n",
      "{}\n",
      "\n",
      "--------------------------\n",
      "\n",
      "9\n",
      "Content: \n",
      "def swiglu(x, alpha: float = 1.702, limit: float = 7.0):\n",
      "    x_glu, x_linear = x[..., ::2], x[..., 1::2]\n",
      "    # Clamp the input values\n",
      "    x_glu = x_glu.clamp(min=None, max=limit)\n",
      "    x_linear = x_linear.clamp(min=-limit, max=limit)\n",
      "    out_glu = x_glu * torch.sigmoid(alpha * x_glu)\n",
      "    # Note we add an extra bias of 1 to the linear layer\n",
      "    return out_glu * (x_linear + 1)\n",
      "-----\n",
      "Metadata: \n",
      "{}\n",
      "\n",
      "--------------------------\n",
      "\n",
      "10\n",
      "Content: \n",
      "class MLPBlock(torch.nn.Module):\n",
      "    def __init__(\n",
      "        self,\n",
      "        config: ModelConfig,\n",
      "        device: torch.device | None = None,\n",
      "    ):\n",
      "        super().__init__()\n",
      "        self.num_experts = config.num_experts\n",
      "        self.experts_per_token = config.experts_per_token\n",
      "        self.swiglu_limit = config.swiglu_limit\n",
      "        self.world_size = dist.get_world_size() if dist.is_initialized() else 1\n",
      "        self.norm = RMSNorm(config.hidden_size, device=device)\n",
      "        self.gate = torch.nn.Linear(\n",
      "            config.hidden_size, config.num_experts, device=device, dtype=torch.bfloat16\n",
      "        )\n",
      "        assert config.intermediate_size % self.world_size == 0\n",
      "        self.mlp1_weight = torch.nn.Parameter(\n",
      "            torch.empty(\n",
      "                (\n",
      "                    config.num_experts,\n",
      "                    config.intermediate_size * 2 // self.world_size,\n",
      "                    config.hidden_size,\n",
      "                ),\n",
      "                device=device,\n",
      "                dtype=torch.bfloat16,\n",
      "            )\n",
      "        )\n",
      "        self.mlp1_bias = torch.nn.Parameter(\n",
      "            torch.empty(\n",
      "                (config.num_experts, config.intermediate_size * 2 // self.world_size),\n",
      "                device=device,\n",
      "                dtype=torch.bfloat16,\n",
      "            )\n",
      "        )\n",
      "        self.mlp2_weight = torch.nn.Parameter(\n",
      "            torch.empty(\n",
      "                (\n",
      "                    config.num_experts,\n",
      "                    config.hidden_size,\n",
      "-----\n",
      "Metadata: \n",
      "{}\n",
      "\n",
      "--------------------------\n",
      "\n",
      "11\n",
      "Content: \n",
      "device=device,\n",
      "                dtype=torch.bfloat16,\n",
      "            )\n",
      "        )\n",
      "        self.mlp2_weight = torch.nn.Parameter(\n",
      "            torch.empty(\n",
      "                (\n",
      "                    config.num_experts,\n",
      "                    config.hidden_size,\n",
      "                    config.intermediate_size // self.world_size,\n",
      "                ),\n",
      "                device=device,\n",
      "                dtype=torch.bfloat16,\n",
      "            )\n",
      "        )\n",
      "        self.mlp2_bias = torch.nn.Parameter(\n",
      "            torch.empty(\n",
      "                (config.num_experts, config.hidden_size),\n",
      "                device=device,\n",
      "                dtype=torch.bfloat16,\n",
      "            )\n",
      "        )\n",
      "-----\n",
      "Metadata: \n",
      "{}\n",
      "\n",
      "--------------------------\n",
      "\n",
      "12\n",
      "Content: \n",
      "def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
      "        t = self.norm(x)\n",
      "        g = self.gate(t)\n",
      "        experts = torch.topk(g, k=self.experts_per_token, dim=-1, sorted=True)\n",
      "        expert_weights = torch.nn.functional.softmax(experts.values, dim=1)\n",
      "        expert_indices = experts.indices\n",
      "\n",
      "        # MLP #1\n",
      "        mlp1_weight = self.mlp1_weight[expert_indices, ...]\n",
      "        mlp1_bias = self.mlp1_bias[expert_indices, ...]\n",
      "        t = torch.einsum(\"beck,bk->bec\", mlp1_weight, t) + mlp1_bias\n",
      "        t = swiglu(t, limit=self.swiglu_limit)\n",
      "\n",
      "        # MLP #2\n",
      "        mlp2_weight = self.mlp2_weight[expert_indices, ...]\n",
      "        mlp2_bias = self.mlp2_bias[expert_indices, ...]\n",
      "        t = torch.einsum(\"beck,bek->bec\", mlp2_weight, t)\n",
      "        if self.world_size > 1:\n",
      "            dist.all_reduce(t, op=dist.ReduceOp.SUM)\n",
      "        t += mlp2_bias\n",
      "\n",
      "        # Weighted sum of experts\n",
      "        t = torch.einsum(\"bec,be->bc\", t, expert_weights)\n",
      "\n",
      "        return x + t\n",
      "-----\n",
      "Metadata: \n",
      "{}\n",
      "\n",
      "--------------------------\n",
      "\n",
      "13\n",
      "Content: \n",
      "class TransformerBlock(torch.nn.Module):\n",
      "    def __init__(\n",
      "        self,\n",
      "        config: ModelConfig,\n",
      "        layer_idx: int,\n",
      "        device: torch.device | None = None,\n",
      "    ):\n",
      "        super().__init__()\n",
      "        self.layer_idx = layer_idx\n",
      "        self.attn = AttentionBlock(config, layer_idx, device)\n",
      "        self.mlp = MLPBlock(config, device)\n",
      "\n",
      "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
      "        x = self.attn(x)\n",
      "        x = self.mlp(x)\n",
      "        return x\n",
      "-----\n",
      "Metadata: \n",
      "{}\n",
      "\n",
      "--------------------------\n",
      "\n",
      "14\n",
      "Content: \n",
      "class Transformer(torch.nn.Module):\n",
      "    def __init__(\n",
      "        self,\n",
      "        config: ModelConfig,\n",
      "        device: torch.device | None = None,\n",
      "    ):\n",
      "        super().__init__()\n",
      "        self.embedding = torch.nn.Embedding(\n",
      "            config.vocab_size, config.hidden_size, device=device, dtype=torch.bfloat16\n",
      "        )\n",
      "        self.block = torch.nn.ModuleList(\n",
      "            [\n",
      "                TransformerBlock(config, layer_idx, device)\n",
      "                for layer_idx in range(config.num_hidden_layers)\n",
      "            ]\n",
      "        )\n",
      "        self.norm = RMSNorm(config.hidden_size, device=device)\n",
      "        self.unembedding = torch.nn.Linear(\n",
      "            config.hidden_size,\n",
      "            config.vocab_size,\n",
      "            bias=False,\n",
      "            device=device,\n",
      "            dtype=torch.bfloat16,\n",
      "        )\n",
      "\n",
      "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
      "        x = self.embedding(x)\n",
      "        for block in self.block:\n",
      "            x = block(x)\n",
      "        x = self.norm(x)\n",
      "        x = self.unembedding(x)\n",
      "        return x\n",
      "\n",
      "    @staticmethod\n",
      "    def from_checkpoint(\n",
      "        path: str, device: str | torch.device = \"cuda\"\n",
      "    ) -> \"Transformer\":\n",
      "        if not isinstance(device, torch.device):\n",
      "            device = torch.device(device)\n",
      "\n",
      "        config_path = os.path.join(path, \"config.json\")\n",
      "        with open(config_path, \"r\") as f:\n",
      "            json_config = json.load(f)\n",
      "            config = ModelConfig(**json_config)\n",
      "-----\n",
      "Metadata: \n",
      "{}\n",
      "\n",
      "--------------------------\n",
      "\n",
      "15\n",
      "Content: \n",
      "config_path = os.path.join(path, \"config.json\")\n",
      "        with open(config_path, \"r\") as f:\n",
      "            json_config = json.load(f)\n",
      "            config = ModelConfig(**json_config)\n",
      "\n",
      "        model = Transformer(\n",
      "            config=config,\n",
      "            device=device,\n",
      "        )\n",
      "        model.eval()\n",
      "\n",
      "        # Load weights\n",
      "        my_rank = dist.get_rank() if dist.is_initialized() else 0\n",
      "        world_size = dist.get_world_size() if dist.is_initialized() else 1\n",
      "        per_rank_intermediate_size = config.intermediate_size // world_size\n",
      "\n",
      "        checkpoint = Checkpoint(path, device)\n",
      "\n",
      "        for name, param in model.named_parameters():\n",
      "            loaded_tensor = checkpoint.get(name)\n",
      "-----\n",
      "Metadata: \n",
      "{}\n",
      "\n",
      "--------------------------\n",
      "\n",
      "16\n",
      "Content: \n",
      "checkpoint = Checkpoint(path, device)\n",
      "\n",
      "        for name, param in model.named_parameters():\n",
      "            loaded_tensor = checkpoint.get(name)\n",
      "\n",
      "            # Note: it would be more efficient to do sharding before upcasting from MXFP4,\n",
      "            # but for simplicity we do it after.\n",
      "            if \"mlp1\" in name:  # both weight and bias\n",
      "                loaded_tensor = loaded_tensor[\n",
      "                    :,\n",
      "                    my_rank * 2\n",
      "                    * per_rank_intermediate_size : (my_rank + 1) * 2\n",
      "                    * per_rank_intermediate_size,\n",
      "                    ...,\n",
      "                ]\n",
      "            elif \"mlp2_weight\" in name:  # only weight\n",
      "                loaded_tensor = loaded_tensor[\n",
      "                    ...,\n",
      "                    my_rank\n",
      "                    * per_rank_intermediate_size : (my_rank + 1)\n",
      "                    * per_rank_intermediate_size,\n",
      "                ]\n",
      "            try:\n",
      "                param.data.copy_(loaded_tensor)\n",
      "            except:\n",
      "                print(f\"{name=} {param.data.shape=} {loaded_tensor.shape=}\")\n",
      "                raise\n",
      "\n",
      "        return model\n",
      "-----\n",
      "Metadata: \n",
      "{}\n",
      "\n",
      "--------------------------\n",
      "\n",
      "17\n",
      "Content: \n",
      "class TokenGenerator:\n",
      "    @torch.inference_mode()\n",
      "    def __init__(self, checkpoint: str, device: torch.device):\n",
      "        self.device = device\n",
      "        self.model = Transformer.from_checkpoint(checkpoint, device=self.device)\n",
      "\n",
      "    @torch.inference_mode()\n",
      "    def generate(self,\n",
      "                 prompt_tokens: list[int],\n",
      "                 stop_tokens: list[int],\n",
      "                 temperature: float = 1.0,\n",
      "                 max_tokens: int = 0,\n",
      "                 return_logprobs: bool = False):\n",
      "        tokens = list(prompt_tokens)\n",
      "        num_generated_tokens = 0\n",
      "        while max_tokens == 0 or num_generated_tokens < max_tokens:\n",
      "            logits = self.model(torch.as_tensor(tokens, dtype=torch.int32, device=self.device))[-1]\n",
      "            if temperature == 0.0:\n",
      "                predicted_token = torch.argmax(logits, dim=-1).item()\n",
      "            else:\n",
      "                probs = torch.softmax(logits * (1.0 / temperature), dim=-1)\n",
      "                predicted_token = torch.multinomial(probs, num_samples=1).item()\n",
      "            tokens.append(predicted_token)\n",
      "            num_generated_tokens += 1\n",
      "\n",
      "            if return_logprobs:\n",
      "                logprobs = torch.log_softmax(logits, dim=-1)\n",
      "                selected_logprobs = logprobs[predicted_token].item()\n",
      "                yield predicted_token, selected_logprobs\n",
      "            else:\n",
      "                yield predicted_token\n",
      "\n",
      "            if predicted_token in stop_tokens:\n",
      "                break\n",
      "-----\n",
      "Metadata: \n",
      "{}\n",
      "\n",
      "--------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "splitter = get_splitter()\n",
    "chunks = split_code(source_code, splitter)\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(i)\n",
    "    print(f'Content: \\n{chunk.page_content}')\n",
    "    print('-----')\n",
    "    print(f'Metadata: \\n{chunk.metadata}')\n",
    "    print('\\n--------------------------\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
